{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Language Model [Thai-Eng]\n",
    "\n",
    "Test Step\n",
    "\n",
    "1. Test ด้วย String ที่เราสร้างเอง\n",
    "    - Thai\n",
    "    - Eng\n",
    "    - Thai + Eng\n",
    "2. Test ด้วยผล OCR\n",
    "    - เอาคำที่ไม่ใช่คำออกไป\n",
    "    - วัด Accuracy, Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk import FreqDist\n",
    "from math import e\n",
    "from math import log\n",
    "from pythainlp import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thai Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75582"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pythainlp.corpus.wordnet import all_lemma_names\n",
    "len(list(all_lemma_names()))\n",
    "thai_corpus = [word for word in list(all_lemma_names()) \\\n",
    "                if word[0].isalpha() and '-' not in word and '.' not in word and '_' not in word]\n",
    "len(thai_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* คำมั่วๆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eng Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77578"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_corpus = [word for word in list(wordnet.all_lemma_names()) \\\n",
    "               if word[0].isalpha() and '-' not in word and '.' not in word and '_' not in word]\n",
    "len(eng_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "### Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thai Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### เตรียมข้อมูลของ ThaiCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tha_trigrams_list = [tri for word in thai_corpus for tri in list(ngrams(word, \n",
    "                                                                        3, \n",
    "                                                                        pad_left=True,\n",
    "                                                                        pad_right=True, \n",
    "                                                                        left_pad_symbol='<s>',\n",
    "                                                                        right_pad_symbol='</s>'))]\n",
    "fdist_tha = FreqDist(tha_trigrams_list)\n",
    "tha_trigrams = {k:v/len(tha_trigrams_list) for k,v in dict(fdist_tha).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### เตรียมข้อมูลภาษาไทย มั่วๆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tha_1_list = [tri for word in thai_1 for tri in list(ngrams(word, \n",
    "                                                            3, \n",
    "                                                            pad_left=True,\n",
    "                                                            pad_right=True, \n",
    "                                                            left_pad_symbol='<s>',\n",
    "                                                            right_pad_symbol='</s>'))]\n",
    "fdist_tha_1 = FreqDist(tha_1_list)\n",
    "tha_1_trigrams = {k:v/len(tha_1_list) for k,v in dict(fdist_tha_1).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### เตรียมข้อมูล Thai wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('wiki.txt', mode='r', encoding='utf-8')\n",
    "wiki = word_tokenize(text.read(), engine=\"attacut\")\n",
    "wiki = [w for w in wiki if w != ' ']\n",
    "\n",
    "tha_wiki_list = [tri for word in wiki for tri in list(ngrams(word, \n",
    "                                                            3, \n",
    "                                                            pad_left=True,\n",
    "                                                            pad_right=True, \n",
    "                                                            left_pad_symbol='<s>',\n",
    "                                                            right_pad_symbol='</s>'))]\n",
    "\n",
    "fdist_tha_wiki = FreqDist(tha_wiki_list)\n",
    "tha_1_trigrams = {k:v/len(tha_wiki_list) for k,v in dict(fdist_tha_wiki).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(word, dict_trigrams):\n",
    "    l = []\n",
    "    ans = []\n",
    "    trigrams_list = list(ngrams(word, 3, pad_left=True,pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "    for i in range(len(trigrams_list)):\n",
    "        try:\n",
    "            l.append(np.log(dict_trigrams[trigrams_list[i]]))\n",
    "            ans.append(trigrams_list[i])\n",
    "        except:\n",
    "#             dict_trigrams[trigrams_list[i]] = 1/len(dict_trigrams)\n",
    "            l.append(np.log(1/len(dict_trigrams)))\n",
    "            ans.append(trigrams_list[i])\n",
    "#     return np.power(e, -sum(l)/len(trigrams_list))\n",
    "#     return sum(l)/len(word)\n",
    "    return (sum(l)/len(word)) , l, ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_like(word, dict_trigrams):\n",
    "    l = []\n",
    "    trigrams_list = list(ngrams(word, 3, pad_left=True,pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "    for i in range(len(trigrams_list)):\n",
    "        try:\n",
    "            l.append(np.log(dict_trigrams[trigrams_list[i]]))\n",
    "        except:\n",
    "            l.append(np.log(1/len(dict_trigrams)))\n",
    "            \n",
    "    return sum(l)/len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(corpus, trigrams):\n",
    "    a = []\n",
    "    for i in corpus:\n",
    "        avg = cal_like(i, trigrams)\n",
    "        a.append(avg)\n",
    "    return sum(a)/len(thai_corpus)\n",
    "#     return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ลองกับ ThaiCorpus ที่เป็นคำที่มีจริงๆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.33581442828286"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = avg(thai_corpus, tha_trigrams)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ลองกับคำมั่วๆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('กฟกพๆๆไำๆ',\n",
       " [(('<s>', '<s>', 'ก'), -4.9571017597635665),\n",
       "  (('<s>', 'ก', 'ฟ'), -10.462588922467008),\n",
       "  (('ก', 'ฟ', 'ก'), -10.462588922467008),\n",
       "  (('ฟ', 'ก', 'พ'), -10.462588922467008),\n",
       "  (('ก', 'พ', 'ๆ'), -10.462588922467008),\n",
       "  (('พ', 'ๆ', 'ๆ'), -10.462588922467008),\n",
       "  (('ๆ', 'ๆ', 'ไ'), -10.462588922467008),\n",
       "  (('ๆ', 'ไ', 'ำ'), -10.462588922467008),\n",
       "  (('ไ', 'ำ', 'ๆ'), -10.462588922467008),\n",
       "  (('ำ', 'ๆ', '</s>'), -13.198014301452465),\n",
       "  (('ๆ', '</s>', '</s>'), -8.431575967868252)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, l, ans = perplexity(thai_1[0], tha_trigrams)\n",
    "thai_1[0], list(zip(ans,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00418356540152761"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = avg(thai_1, tha_trigrams)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10152193330441954"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = avg(wiki, tha_trigrams)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eng Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "819776"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_trigrams = [tri for word in eng_corpus for tri in list(ngrams(word, 3, pad_left=True,\\\n",
    "                                                                        pad_right=True, left_pad_symbol='<s>',\\\n",
    "                                                                        right_pad_symbol='</s>'))]\n",
    "len(eng_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = ['an apple', 'an orange']\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in train_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2\n",
    "train_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "words = [word for sent in tokenized_text for word in sent]\n",
    "words.extend([\"<s>\", \"</s>\"])\n",
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_vocab = Vocabulary(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLE(n)\n",
    "model.fit(train_data, padded_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sentences = ['an apple', 'an ant']\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Estimates: [(('an', ('<s>',)), 1.0), (('apple', ('an',)), 0.5), (('</s>', ('apple',)), 1.0)]\n",
      "MLE Estimates: [(('an', ('<s>',)), 1.0), (('ant', ('an',)), 0.0), (('</s>', ('ant',)), 0)]\n"
     ]
    }
   ],
   "source": [
    "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "for test in test_data:\n",
    "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])) for ngram in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP(an apple):1.2599210498948732\n",
      "PP(an ant):inf\n"
     ]
    }
   ],
   "source": [
    "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "for i, test in enumerate(test_data):\n",
    "    print(\"PP({0}):{1}\".format(test_sentences[i], model.perplexity(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Language Model @1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [char for word in thai_corpus for char in word]\n",
    "# words.extend([\"<s>\", \"</s>\"])\n",
    "words.append(\"<s>\")\n",
    "words.append(\"</s>\")\n",
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_vocab = Vocabulary(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Ngram <<s>> isn't a tuple, but <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-43a692cab258>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtha_trigrams_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, text, vocabulary_text)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 )\n\u001b[0;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\counter.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, ngram_text)\u001b[0m\n\u001b[0;32m    123\u001b[0m                     raise TypeError(\n\u001b[0;32m    124\u001b[0m                         \u001b[1;34m\"Ngram <{0}> isn't a tuple, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m                         \u001b[1;34m\"but {1}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m                     )\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Ngram <<s>> isn't a tuple, but <class 'str'>"
     ]
    }
   ],
   "source": [
    "model = MLE(n)\n",
    "model.fit(tha_trigrams_list, padded_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python37564bitnlpcondae71441629d68476b8ab8fd18fb52de04"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
